#!/usr/bin/env python3
# LICENSE
# This software is the exclusive property of Gencovery SAS.
# The use and distribution of this software is prohibited without the prior consent of Gencovery SAS.
# About us: https://gencovery.com

import os
from pathlib import Path
from typing import Final

from gws_core import (
    BoolParam,
    ConfigParams,
    ConfigSpecs,
    File,
    Folder,
    InputSpec,
    InputSpecs,
    IntParam,
    OutputSpec,
    OutputSpecs,
    ResourceSet,
    ShellProxy,
    StrParam,
    TableImporter,
    Task,
    TaskInputs,
    TaskOutputs,
    task_decorator,
)

from ..base_env.Bakta_env import BaktaShellProxyHelper

# ----------------- helpers -----------------

def _find_bakta_db_dir(base: Path) -> Path | None:
    """Return a folder containing 'version.json' among base, base/db, base/db-full, base/db-light."""
    for c in (base, base / "db", base / "db-full", base / "db-light"):
        if (c / "version.json").is_file():
            return c
    return None


def _safe_token(x: str) -> str:
    """Uppercase alnum token for locus/locus-tag building."""
    t = "".join(ch for ch in (x or "") if ch.isalnum()).upper()
    return t if t else "SEQ"


def _parse_gff_attributes(attr_field: str) -> dict:
    d = {}
    if not attr_field:
        return d
    for kv in attr_field.split(";"):
        if not kv:
            continue
        if "=" in kv:
            k, v = kv.split("=", 1)
            d[k.strip()] = v.strip()
        else:
            d[kv.strip()] = ""
    return d


def _read_fasta_lengths(fp: Path) -> list[tuple[str, int]]:
    """Quick FASTA pass: [(contig_id, length), ...]."""
    rows, cur_id, cur_len = [], None, 0
    with fp.open("rt", encoding="utf-8", errors="ignore") as fh:
        for line in fh:
            if line.startswith(">"):
                if cur_id is not None:
                    rows.append((cur_id, cur_len))
                cur_id = line[1:].strip().split()[0]
                cur_len = 0
            else:
                cur_len += len(line.strip())
        if cur_id is not None:
            rows.append((cur_id, cur_len))
    return rows


def _write_replicons_for_all(
    fasta_path: Path, out_dir: Path, rtype: str | None, rtopo: str | None
) -> Path | None:
    """
    If 'rtype' (chromosome|plasmid) or 'rtopo' (circular|linear) are provided,
    create replicons_all.tsv declaring the same for all contigs.
    Columns: original_id,length,new_id,type,topology,name
    """
    rtype = (rtype or "").strip().lower()
    rtopo = (rtopo or "").strip().lower()
    if not rtype and not rtopo:
        return None
    if rtype and rtype not in {"chromosome", "plasmid"}:
        print(f"[WARN] Unknown replicon type '{rtype}', ignoring type.")
        rtype = ""
    if rtopo and rtopo not in {"circular", "linear"}:
        print(f"[WARN] Unknown replicon topology '{rtopo}', ignoring topology.")
        rtopo = ""
    if not rtype and not rtopo:
        return None

    rows = _read_fasta_lengths(fasta_path)
    if not rows:
        print("[WARN] No contigs found to build replicons.tsv")
        return None

    tsv = out_dir / "replicons_all.tsv"
    with tsv.open("wt", encoding="utf-8") as w:
        w.write("original_id\tlength\tnew_id\ttype\ttopology\tname\n")
        for cid, clen in rows:
            w.write(f"{cid}\t{clen}\t{cid}\t{rtype}\t{rtopo}\t\n")
    return tsv


# descriptive → NCBI code
TRANSLATION_TABLE_CHOICES: Final[list[str]] = [
    "The Bacterial, Archaeal and Plant Plastid Code",  # → 11
    "The Mold, Protozoan, and Coelenterate Mitochondrial Code and the Mycoplasma/Spiroplasma Code",  # → 4
]
TRANSLATION_TABLE_MAP: Final[dict] = {
    TRANSLATION_TABLE_CHOICES[0]: "11",
    TRANSLATION_TABLE_CHOICES[1]: "4",
}


# ----------------- task -----------------

@task_decorator(
    "Bakta",
    human_name="Prokaryotes Genome Annotation",
    short_description="Annotate bacterial genomes/plasmids locally with Bakta"
)
class BaktaTask(Task):
    """
    Bakta rapidly and consistently annotates bacterial genomes and plasmids (isolates and MAGs).
    It predicts CDS and non-coding RNAs (tRNA, rRNA, tmRNA, ncRNA), detects CRISPR and replication origins (oriC/V),
    and adds rich database cross-references (RefSeq WP_*, UniRef100, UniParc) to support FAIR, downstream analyses.
    Results come in standard, machine-readable formats (GFF3/GBFF/EMBL/FASTA/JSON/TSV) ready for pipelines and visualization.

    Outputs include:
      - TSV — human-readable annotation table.
      - GFF3 — annotations + sequences.
      - GBFF / EMBL — multi-record formats.
      - FFN / FAA / FNA — FASTA sequences (features, proteins, contigs).
      - JSON — full internal annotation.
      - hypotheticals.tsv / inference.tsv / hypotheticals.faa.
      - PNG / SVG circular plot (if generated by Bakta).
      - TXT summary.
    """

    # Inputs
    input_specs: Final[InputSpecs] = InputSpecs({
        "genome_fasta": InputSpec(
            File, human_name="Genome FASTA (contigs)",
            short_description="Assembly FASTA to annotate (chromosome/plasmids/MAGs)"
        ),
        "bakta_db_folder": InputSpec(
            Folder, human_name="Bakta DB folder",
            short_description="Folder created by the Bakta DB task (or its db/db-full/db-light child)"
        ),
    })

    # Outputs (ResourceSet only)
    output_specs: Final[OutputSpecs] = OutputSpecs({
        "bakta_outputs": OutputSpec(ResourceSet, human_name="Bakta outputs")
    })

    # Config
    config_specs: Final[ConfigSpecs] = ConfigSpecs({
        # Output naming (optional)
        "prefix": StrParam(default_value="", short_description="Output file prefix (default: FASTA stem)"),

        # Organism metadata (optional)
        "genus":   StrParam(default_value="", short_description="Genus (optional)"),
        "species": StrParam(default_value="", short_description="Species (optional)"),
        "strain":  StrParam(default_value="", short_description="Strain (optional)"),

        # Flags
        "complete_genome": BoolParam(default_value=False, short_description="All sequences are complete"),
        "keep_headers":    BoolParam(default_value=False, short_description="Keep original contig headers"),
        "compliant":       BoolParam(default_value=False, short_description="INSDC compliant output"),

        # Filters / settings
        "min_contig_len": IntParam(default_value=1, min_value=1, short_description="Minimum contig length"),
        "translation_table": StrParam(
            default_value=TRANSLATION_TABLE_CHOICES[0],
            allowed_values=TRANSLATION_TABLE_CHOICES,
            short_description=(
                "Genetic code used for translation:\n"
                f"- {TRANSLATION_TABLE_CHOICES[0]} (NCBI 11)\n"
                f"- {TRANSLATION_TABLE_CHOICES[1]} (NCBI 4)"
            )
        ),
        "gram": StrParam(
            default_value="?", allowed_values=["+", "-", "?"],
            short_description="Gram type: + (diderm), - (monoderm), ? (unknown)"
        ),

        # Global replicon metadata (optional; applied to all contigs if set)
        "replicon_type": StrParam(
            default_value=None, allowed_values=["chromosome", "plasmid", " "],
            short_description="Apply a global replicon Type to all contigs (optional)"
        ),
        "replicon_topology": StrParam(
            default_value=None, allowed_values=["circular", "linear", " "],
            short_description="Apply a global Topology to all contigs (optional)"
        ),

        # Performance
        "threads": IntParam(default_value=8, min_value=1, short_description="CPU threads"),
    })

    def run(self, params: ConfigParams, inputs: TaskInputs) -> TaskOutputs:
        fasta: File        = inputs["genome_fasta"]
        db_folder: Folder  = inputs["bakta_db_folder"]

        shell: ShellProxy = BaktaShellProxyHelper.create_proxy(self.message_dispatcher)
        out_dir = Path(shell.working_dir) / "result"
        out_dir.mkdir(parents=True, exist_ok=True)

        prefix = (params["prefix"] or Path(fasta.path).stem).strip()

        # Enforce INSDC min contig length when compliant
        min_len = int(params["min_contig_len"])
        if params["compliant"] and min_len < 200:
            print(f"[INFO] INSDC compliant: raising min_contig_len {min_len} → 200")
            min_len = 200

        threads = min(params["threads"], os.cpu_count() or 8)

        # Resolve DB directory
        db_dir = _find_bakta_db_dir(Path(db_folder.path))
        if db_dir is None:
            raise RuntimeError("Bakta DB path invalid: could not find version.json in folder or db/db-full/db-light")
        print(f"[INFO] Using Bakta DB at: {db_dir}")

        # Map translation table description → numeric code
        tt = TRANSLATION_TABLE_MAP.get(params["translation_table"], "11")
        gram = params["gram"] if params["gram"] in {"+", "-", "?"} else "?"

        # Auto locus & locus-tag (kept internal; not exposed in UI)
        locus = _safe_token(prefix)[:12]
        lt = (_safe_token(params.get("genus") or "")[:3] + _safe_token(params.get("species") or "")[:3]) or "PROJECT"
        lt = lt[:12]

        # Optional global replicons for all contigs
        replicons = _write_replicons_for_all(
            Path(fasta.path), out_dir,
            params.get("replicon_type"), params.get("replicon_topology")
        )

        # Build Bakta command
        cmd_parts = [
            "bakta",
            f"--db {db_dir}",
            f"--output {out_dir}",
            f"--prefix {prefix}",
            f"--min-contig-length {min_len}",
            f"--threads {threads}",
            f"--translation-table {tt}",
            f"--gram {gram}",
            "--force",
            "--locus", locus,
            "--locus-tag", lt,
        ]
        if replicons:
            cmd_parts += ["--replicons", str(replicons)]
        if params["complete_genome"]:
            cmd_parts.append("--complete")
        if params["keep_headers"]:
            cmd_parts.append("--keep-contig-headers")
        if params["compliant"]:
            cmd_parts.append("--compliant")

        if params.get("genus"):
            cmd_parts += ["--genus", params["genus"]]
        if params.get("species"):
            cmd_parts += ["--species", params["species"]]
        if params.get("strain"):
            cmd_parts += ["--strain", params["strain"]]

        cmd_parts.append(f"\"{fasta.path}\"")  # FASTA last

        cmd = " ".join(cmd_parts)
        print("[DEBUG]", cmd)

        # Run
        if shell.run(cmd, shell_mode=True):
            raise RuntimeError("Bakta annotation failed")

        # ----------------- collect into ResourceSet -----------------
        rs = ResourceSet()
        added_names = set()

        def _add_unique(file_obj: File, name: str):
            """Avoid duplicate resource names in the ResourceSet."""
            if name not in added_names:
                rs.add_resource(file_obj, name)
                added_names.add(name)

        # A) Standardized annotation table from GFF3 (exactly one header row)
        std_cols = ["Sequence", "Type", "Start", "Stop", "Strand", "Locus tag", "Gene", "Product", "DbXrefs"]
        gff_path = out_dir / f"{prefix}.gff3"
        std_out  = out_dir / "annotation_table.tsv"

        if gff_path.is_file():
            rows = []
            with gff_path.open("rt", encoding="utf-8", errors="ignore") as fh:
                for line in fh:
                    if not line or line.startswith("#"):
                        continue
                    parts = line.rstrip("\n").split("\t")
                    if len(parts) < 9:
                        continue
                    seqid, source, ftype, start, end, score, strand, phase, attrs = parts
                    ad = _parse_gff_attributes(attrs)

                    locus_tag = ad.get("locus_tag", "")
                    gene      = ad.get("gene", ad.get("Name", ""))
                    product   = ad.get("product", ad.get("Product", ""))
                    dbx       = ad.get("Dbxref", ad.get("Dbxrefs", ""))

                    rows.append([
                        seqid,
                        ftype,
                        start,
                        end,
                        strand if strand in {"+", "-", "."} else ".",
                        locus_tag,
                        gene,
                        product,
                        dbx
                    ])

            with std_out.open("wt", encoding="utf-8") as w:
                w.write("\t".join(std_cols) + "\n")
                for r in rows:
                    w.write("\t".join(str(x) if x is not None else "" for x in r) + "\n")

            std_table = TableImporter.call(
                File(str(std_out)),
                {"delimiter": "tab", "header": 0, "file_format": "tsv"}
            )
            _add_unique(std_table, "annotation_table.tsv")

        # B) Add common outputs as raw files (avoid duplicates; do not re-add annotation_table.tsv)
        extras = [
            f"{prefix}.gbff", f"{prefix}.gff3", f"{prefix}.embl",
            f"{prefix}.faa",  f"{prefix}.ffn",  f"{prefix}.fna",
            f"{prefix}.hypotheticals.faa", f"{prefix}.hypotheticals.tsv",
            f"{prefix}.inference.tsv", f"{prefix}.json",
            f"{prefix}.png", f"{prefix}.svg",
            f"{prefix}.txt", f"{prefix}.log",
            f"{prefix}.tsv",
        ]
        for name in extras:
            p = out_dir / name
            if p.is_file():
                _add_unique(File(str(p)), name)

        # catch-all (avoid duplicates)
        for p in sorted(out_dir.glob("*")):
            if p.is_file():
                _add_unique(File(str(p)), p.name)

        return {"bakta_outputs": rs}
